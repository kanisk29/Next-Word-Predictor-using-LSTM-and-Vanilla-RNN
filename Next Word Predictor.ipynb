{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1D9hGqJN1zdz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dropout,Dense,BatchNormalization,SimpleRNN,Embedding,LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.utils import plot_model,pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4i-Rs3QI3sIp"
      },
      "outputs": [],
      "source": [
        "with open(\"acc.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read()\n",
        "new_li = [line.strip().lower()\n",
        "          for line in data.split(\"\\n\")\n",
        "          if line.strip() != \"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xvVvS9muOFgA"
      },
      "outputs": [],
      "source": [
        "seq_len = 10\n",
        "def build_sequences(text,seq_len = 5):\n",
        "    token = Tokenizer()\n",
        "    token.fit_on_texts(text)\n",
        "    sequences = []\n",
        "    for line in text:\n",
        "        token_list = token.texts_to_sequences([line])[0]\n",
        "        for i in range(seq_len, len(token_list)):\n",
        "            sequences.append(token_list[i-seq_len:i+1])\n",
        "    sequences = pad_sequences(sequences, maxlen=seq_len+1, padding=\"pre\")\n",
        "    return sequences, token\n",
        "\n",
        "ip_sequences, token = build_sequences(new_li,seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9OCa8K-45XxB"
      },
      "outputs": [],
      "source": [
        "x = ip_sequences[:,:-1]\n",
        "y = ip_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gz7Nux-eNMUN"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fPXnPX6TNSmI"
      },
      "outputs": [],
      "source": [
        "def evaluation_of_model(model_name):\n",
        "  loss = model_name.evaluate(x_test, y_test, verbose=0)\n",
        "  print(f\"Test Loss: {loss:.4f}\")\n",
        "  perplexity = math.exp(loss)\n",
        "  print(f\"Perplexity: {perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ceTvZ7y1PuZk"
      },
      "outputs": [],
      "source": [
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds)\n",
        "    preds = np.log(preds + 1e-8) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return np.random.choice(len(preds), p=preds)\n",
        "index_word = {v:k for k,v in token.word_index.items()}\n",
        "def generate_text(seed_text, model, next_words=10, temperature=0.7):\n",
        "    for _ in range(next_words):\n",
        "        seq = token.texts_to_sequences([seed_text])[0]\n",
        "        seq = pad_sequences([seq], maxlen=seq_len)\n",
        "        preds = model.predict(seq, verbose=0)[0]\n",
        "        next_index = sample_with_temperature(preds, temperature)\n",
        "        next_word = index_word.get(next_index, \"\")\n",
        "        seed_text += \" \" + next_word\n",
        "    return seed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fqQdEk27P1ab"
      },
      "outputs": [],
      "source": [
        "rnn_model = Sequential()\n",
        "rnn_model.add(Embedding(input_dim=len(token.word_index)+1,output_dim = 16))\n",
        "rnn_model.add(SimpleRNN(128))\n",
        "rnn_model.add(Dense(len(token.word_index)+1,activation = 'softmax'))\n",
        "rnn_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')\n",
        "history_rnn = rnn_model.fit(x_train,y_train,epochs = 20,verbose = 0, batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluation_of_model(rnn_model))\n",
        "print(generate_text(\"inheritance values are\",rnn_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO3OwKdJm1gA",
        "outputId": "2adbc10a-eb14-4c77-ab73-c1e1700e9704"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 9.7209\n",
            "Perplexity: 16662.26\n",
            "None\n",
            "inheritance values are often data created obtained context of a phrase schemas in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gurkVdA2RY-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0136ea76-5530-4120-aadd-7abec71c22a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 7.5803\n",
            "Epoch 2/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 6.8639\n",
            "Epoch 3/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 6.7004\n",
            "Epoch 4/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 6.5962\n",
            "Epoch 5/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 6.4521\n",
            "Epoch 6/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 6.3205\n",
            "Epoch 7/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 6.1275\n",
            "Epoch 8/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 5.9789\n",
            "Epoch 9/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 5.8038\n",
            "Epoch 10/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 5.6454\n",
            "Epoch 11/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 5.5169\n",
            "Epoch 12/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 5.3433\n",
            "Epoch 13/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 5.1836\n",
            "Epoch 14/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 4.9938\n",
            "Epoch 15/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.8328\n",
            "Epoch 16/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 4.6540\n",
            "Epoch 17/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 4.5170\n",
            "Epoch 18/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 4.3930\n",
            "Epoch 19/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.2028\n",
            "Epoch 20/20\n",
            "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 4.0224\n"
          ]
        }
      ],
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=len(token.word_index)+1,output_dim = 100))\n",
        "lstm_model.add(LSTM(300,return_sequences=True,dropout=0.2))\n",
        "lstm_model.add(LSTM(300,dropout=0.2))\n",
        "lstm_model.add(Dense(len(token.word_index)+1,activation = 'softmax'))\n",
        "lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')\n",
        "history_lstm = lstm_model.fit(x_train,y_train,epochs = 20,batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NxXIwOoRTOc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1a178b-b244-46bb-ab24-76dfb20f27d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 9.4513\n",
            "Perplexity: 12724.68\n",
            "None\n",
            "the centre of the scientific html content for a topic for the\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_of_model(lstm_model))\n",
        "print(generate_text(\"the centre\",lstm_model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"the billionaire\", lstm_model, temperature=0.3))\n",
        "print(generate_text(\"the billionaire\", lstm_model, temperature=0.7))\n",
        "print(generate_text(\"the billionaire\", lstm_model, temperature=1.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkzwcVzPjIee",
        "outputId": "78e9fd63-735a-4d43-a563-41ffb2f60f45"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the billionaire of the brain’s database did his help in the sea\n",
            "the billionaire country using einstein’s rule and 2004 the indian language and\n",
            "the billionaire country after researchers of the iconic to grow scientific us\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}